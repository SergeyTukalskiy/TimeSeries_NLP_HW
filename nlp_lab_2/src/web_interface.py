import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, Any, List
import sys
import os
import networkx as nx
from sklearn.cluster import KMeans

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.semantic_analysis import SemanticAnalyzer
from src.distributed_models import DistributedModels
from src.dimensionality_reduction import DimensionalityReducer

class VectorSpaceExplorer:
    """–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤"""
    
    def __init__(self):
        self.semantic_analyzer = SemanticAnalyzer()
        self.dimensionality_reducer = DimensionalityReducer()
        self.models = {}
        
    def load_models(self, models_dict: Dict[str, Any]):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        self.models = models_dict
        
    def render_sidebar(self):
        """–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏"""
        st.sidebar.title("üîç –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤")
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
        if self.models:
            model_names = list(self.models.keys())
            selected_model_name = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:", model_names)
            self.selected_model = self.models[selected_model_name]['model']
        else:
            st.sidebar.warning("–ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            self.selected_model = None
            
        # –ù–∞–≤–∏–≥–∞—Ü–∏—è
        page = st.sidebar.radio(
            "–†–∞–∑–¥–µ–ª—ã:",
            ["–í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞", "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ", "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–∏", 
             "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è", "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç"]
        )
        
        return page
    
    def render_vector_arithmetic(self):
        """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏ —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤"""
        st.header("üßÆ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            positive_words = st.text_area(
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):", 
                "–∫–æ—Ä–æ–ª—å, –∂–µ–Ω—â–∏–Ω–∞",
                help="–°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –ø—Ä–∏–±–∞–≤–ª–µ–Ω—ã –∫ –≤–µ–∫—Ç–æ—Ä—É"
            )
        with col2:
            negative_words = st.text_area(
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):", 
                "–º—É–∂—á–∏–Ω–∞",
                help="–°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –≤—ã—á—Ç–µ–Ω—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–∞"
            )
        with col3:
            topn = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:", 1, 20, 10)
        
        if st.button("–í—ã—á–∏—Å–ª–∏—Ç—å"):
            if self.selected_model:
                positive = [w.strip() for w in positive_words.split(',') if w.strip()]
                negative = [w.strip() for w in negative_words.split(',') if w.strip()]
                
                try:
                    results = self.semantic_analyzer.vector_arithmetic(
                        self.selected_model, positive, negative, topn
                    )
                    
                    if results:
                        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ñ–æ—Ä–º—É–ª—É
                        formula_parts = []
                        if positive:
                            formula_parts.append(" + ".join(positive))
                        if negative:
                            formula_parts.append(" - " + " - ".join(negative))
                        
                        formula = "".join(formula_parts)
                        st.subheader(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {formula}")
                        
                        # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        df = pd.DataFrame(results, columns=['–°–ª–æ–≤–æ', '–°—Ö–æ–¥—Å—Ç–≤–æ'])
                        st.dataframe(df)
                        
                        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                        fig = px.bar(df, x='–°—Ö–æ–¥—Å—Ç–≤–æ', y='–°–ª–æ–≤–æ', orientation='h',
                                    title=f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏: {formula}")
                        st.plotly_chart(fig)
                        
                        # –ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
                        st.subheader("üîç –ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö —Å–ª–æ–≤")
                        
                        # –î–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
                        if positive:
                            st.write("**–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞:**")
                            pos_cols = st.columns(len(positive))
                            for i, word in enumerate(positive):
                                with pos_cols[i]:
                                    neighbors = self.semantic_analyzer.find_similar_words(
                                        self.selected_model, word, 5
                                    )
                                    if neighbors:
                                        st.write(f"–ë–ª–∏–∂–∞–π—à–∏–µ –∫ '{word}':")
                                        neighbor_df = pd.DataFrame(neighbors, columns=['–°–ª–æ–≤–æ', '–°—Ö–æ–¥—Å—Ç–≤–æ'])
                                        st.dataframe(neighbor_df)
                        
                        # –î–ª—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
                        if negative:
                            st.write("**–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞:**")
                            neg_cols = st.columns(len(negative))
                            for i, word in enumerate(negative):
                                with neg_cols[i]:
                                    neighbors = self.semantic_analyzer.find_similar_words(
                                        self.selected_model, word, 5
                                    )
                                    if neighbors:
                                        st.write(f"–ë–ª–∏–∂–∞–π—à–∏–µ –∫ '{word}':")
                                        neighbor_df = pd.DataFrame(neighbors, columns=['–°–ª–æ–≤–æ', '–°—Ö–æ–¥—Å—Ç–≤–æ'])
                                        st.dataframe(neighbor_df)
                        
                    else:
                        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–ø–µ—Ä–∞—Ü–∏—é. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —Å–ª–æ–≤–∞ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏.")
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏: {e}")
            else:
                st.error("–ú–æ–¥–µ–ª—å –Ω–µ –≤—ã–±—Ä–∞–Ω–∞")
    
    def render_semantic_similarity(self):
        """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å –≥—Ä–∞—Ñ–∞–º–∏ –∏ –∞–Ω–∞–ª–∏–∑–æ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π"""
        st.header("üìä –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º")
        
        tab1, tab2, tab3 = st.tabs(["–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π", "–ì—Ä–∞—Ñ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π", "–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π"])
        
        with tab1:
            col1, col2 = st.columns(2)
            
            with col1:
                word1 = st.text_input("–ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ:", "–∫–æ–º–ø—å—é—Ç–µ—Ä")
            with col2:
                word2 = st.text_input("–í—Ç–æ—Ä–æ–µ —Å–ª–æ–≤–æ:", "–Ω–æ—É—Ç–±—É–∫")
            
            if st.button("–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ") and self.selected_model:
                try:
                    similarity = self.semantic_analyzer.calculate_word_distance(
                        self.selected_model, word1, word2
                    )
                    
                    st.metric("–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ", f"{similarity:.4f}")
                    
                    # –ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ –¥–ª—è –æ–±–æ–∏—Ö —Å–ª–æ–≤
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        neighbors1 = self.semantic_analyzer.find_similar_words(
                            self.selected_model, word1, 10
                        )
                        st.subheader(f"–ë–ª–∏–∂–∞–π—à–∏–µ –∫ '{word1}':")
                        df1 = pd.DataFrame(neighbors1, columns=['–°–ª–æ–≤–æ', '–°—Ö–æ–¥—Å—Ç–≤–æ'])
                        st.dataframe(df1)
                    
                    with col2:
                        neighbors2 = self.semantic_analyzer.find_similar_words(
                            self.selected_model, word2, 10
                        )
                        st.subheader(f"–ë–ª–∏–∂–∞–π—à–∏–µ –∫ '{word2}':")
                        df2 = pd.DataFrame(neighbors2, columns=['–°–ª–æ–≤–æ', '–°—Ö–æ–¥—Å—Ç–≤–æ'])
                        st.dataframe(df2)
                        
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞: {e}")
        
        with tab2:
            seed_words = st.text_area(
                "–ù–∞—á–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –≥—Ä–∞—Ñ–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):",
                "–∫–æ–º–ø—å—é—Ç–µ—Ä, –ø—Ä–æ–≥—Ä–∞–º–º–∞, –¥–∞–Ω–Ω—ã–µ, –∞–ª–≥–æ—Ä–∏—Ç–º, —Å–µ—Ç—å, –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π, –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
            )
            threshold = st.slider("–ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è —Å–≤—è–∑–µ–π:", 0.1, 0.9, 0.5)
            depth = st.slider("–ì–ª—É–±–∏–Ω–∞ –≥—Ä–∞—Ñ–∞:", 1, 3, 2)
            
            if st.button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ") and self.selected_model:
                seed_list = [w.strip() for w in seed_words.split(',') if w.strip()]
                
                network = self.semantic_analyzer.create_semantic_network(
                    self.selected_model, seed_list, depth=depth, threshold=threshold
                )
                
                st.metric("–£–∑–ª—ã –≥—Ä–∞—Ñ–∞", network['metrics']['nodes_count'])
                st.metric("–°–≤—è–∑–∏ –≥—Ä–∞—Ñ–∞", network['metrics']['edges_count'])
                st.metric("–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≥—Ä–∞—Ñ–∞", f"{network['metrics']['density']:.4f}")
                
                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å–≤—è–∑–µ–π
                links_df = pd.DataFrame(network['links'])
                if not links_df.empty:
                    st.subheader("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ (—Ç–æ–ø-20 –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É):")
                    st.dataframe(links_df.nlargest(20, 'value'))
                else:
                    st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–≤—è–∑–µ–π —Å –∑–∞–¥–∞–Ω–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º —Å—Ö–æ–¥—Å—Ç–≤–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å –ø–æ—Ä–æ–≥.")
        
        with tab3:
            st.info("–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –≤—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É —Å–ª—É—á–∞–π–Ω—ã–º–∏ –ø–∞—Ä–∞–º–∏ —Å–ª–æ–≤ –∏–∑ —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏.")
            
            if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π") and self.selected_model:
                with st.spinner("–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π..."):
                    distribution = self.semantic_analyzer.analyze_distance_distribution(
                        self.selected_model
                    )
                    
                    st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π")
                    col1, col2, col3, col4 = st.columns(4)
                    col1.metric("–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ", f"{distribution['mean']:.4f}")
                    col2.metric("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ", f"{distribution['std']:.4f}")
                    col3.metric("–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ", f"{distribution['min']:.4f}")
                    col4.metric("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ", f"{distribution['max']:.4f}")
                    
                    st.write(f"**–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–∞—Ä:** {distribution['n_pairs']}")
                    st.write(f"**–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏:** {distribution.get('sample_size', 'N/A')}")
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                    if distribution['distances']:
                        fig = px.histogram(
                            x=distribution['distances'], 
                            title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π",
                            labels={'x': '–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ', 'y': '–ß–∞—Å—Ç–æ—Ç–∞'},
                            nbins=20
                        )
                        fig.add_vline(x=distribution['mean'], line_dash="dash", line_color="red", 
                                    annotation_text=f"–°—Ä–µ–¥–Ω–µ–µ: {distribution['mean']:.4f}")
                        st.plotly_chart(fig)
                    
                    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    st.subheader("üìà –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    mean_similarity = distribution['mean']
                    
                    if mean_similarity > 0.3:
                        st.success("‚úÖ –í—ã—Å–æ–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: –º–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏")
                    elif mean_similarity > 0.1:
                        st.info("‚ÑπÔ∏è –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: –º–æ–¥–µ–ª—å —Ä–∞–∑–ª–∏—á–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É, –Ω–æ –µ—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è")
                    else:
                        st.warning("‚ö†Ô∏è –ù–∏–∑–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: –º–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏")
    
    def render_semantic_axes(self):
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–µ–π"""
        st.header("üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–µ–π")
        
        st.info("""
        **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–∏** –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –º–µ–∂–¥—É –¥–≤—É–º—è –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏.
        - **–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**: –±–ª–∏–∂–µ –∫ –ø–µ—Ä–≤–æ–º—É —Å–ª–æ–≤—É
        - **–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**: –±–ª–∏–∂–µ –∫–æ –≤—Ç–æ—Ä–æ–º—É —Å–ª–æ–≤—É
        - **–ù—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**: –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ—É–¥–∞–ª–µ–Ω–Ω—ã–µ
        """)
        
        # –¢–ï–°–¢–ò–†–£–ï–ú –†–ê–ó–ù–´–ï –û–°–ò
        axis_options = {
            "–ì–µ–Ω–¥–µ—Ä–Ω–∞—è": ("–º—É–∂—á–∏–Ω–∞", "–∂–µ–Ω—â–∏–Ω–∞"),
            "–í—Ä–µ–º—è": ("–ø—Ä–æ—à–ª–æ–µ", "–±—É–¥—É—â–µ–µ"),
            "–û—Ü–µ–Ω–∫–∞": ("–ø–ª–æ—Ö–æ–π", "—Ö–æ—Ä–æ—à–∏–π"),
            "–†–∞–∑–º–µ—Ä": ("–º–∞–ª–µ–Ω—å–∫–∏–π", "–±–æ–ª—å—à–æ–π"),
        }
        
        selected_axis = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –æ—Å—å:", list(axis_options.keys()))
        
        word1, word2 = axis_options[selected_axis]
        st.write(f"**–û—Å—å:** {word1} ‚Üê‚Üí {word2}")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –æ—Å–µ–π
        test_words_config = {
            "–ì–µ–Ω–¥–µ—Ä–Ω–∞—è": "–º—É–∂—á–∏–Ω–∞, –∂–µ–Ω—â–∏–Ω–∞, –ø–∞—Ä–µ–Ω—å, –¥–µ–≤—É—à–∫–∞, –æ—Ç–µ—Ü, –º–∞—Ç—å, —Å—ã–Ω, –¥–æ—á—å, –±—Ä–∞—Ç, —Å–µ—Å—Ç—Ä–∞, –¥—è–¥—è, —Ç–µ—Ç—è",
            "–í—Ä–µ–º—è": "–≤—á–µ—Ä–∞, —Å–µ–≥–æ–¥–Ω—è, –∑–∞–≤—Ç—Ä–∞, —Å—Ç–∞—Ä—ã–π, –Ω–æ–≤—ã–π, –¥—Ä–µ–≤–Ω–∏–π, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π, –ø—Ä–æ—à–ª—ã–π, –±—É–¥—É—â–∏–π, –Ω–∞—Å—Ç–æ—è—â–∏–π",
            "–û—Ü–µ–Ω–∫–∞": "—É–∂–∞—Å–Ω—ã–π, –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π, –æ—Ç–ª–∏—á–Ω—ã–π, –ø–ª–æ—Ö–æ–π, —Ö–æ—Ä–æ—à–∏–π, —Å–∫–≤–µ—Ä–Ω—ã–π, –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π, —Ö—É–¥—à–∏–π, –ª—É—á—à–∏–π",
            "–†–∞–∑–º–µ—Ä": "–º–∞–ª—ã–π, –∫—Ä—É–ø–Ω—ã–π, –æ–≥—Ä–æ–º–Ω—ã–π, –∫—Ä–æ—à–µ—á–Ω—ã–π, –≥–∏–≥–∞–Ω—Ç—Å–∫–∏–π, –º–∏–Ω–∏–∞—Ç—é—Ä–Ω—ã–π, –º–∞—Å—à—Ç–∞–±–Ω—ã–π, –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π",
        }
        
        test_words = st.text_area(
            "–°–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:",
            test_words_config.get(selected_axis, "—Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2, —Å–ª–æ–≤–æ3")
        )
        
        col1, col2 = st.columns(2)
        with col1:
            use_advanced = st.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥", value=True)
        with col2:
            show_debug = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é")
        
        if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Å—å") and self.selected_model:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤ –æ—Å–∏
            missing_axis_words = []
            for word in [word1, word2]:
                if word not in self.selected_model.wv.key_to_index:
                    missing_axis_words.append(word)
            
            if missing_axis_words:
                st.error(f"–°–ª–æ–≤–∞ –æ—Å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –º–æ–¥–µ–ª–∏: {', '.join(missing_axis_words)}")
                available_words = list(self.selected_model.wv.key_to_index.keys())[:20]
                st.info("–î–æ—Å—Ç—É–ø–Ω—ã–µ —Å–ª–æ–≤–∞ (–ø–µ—Ä–≤—ã–µ 20): " + ", ".join(available_words))
                return
            
            axis_pairs = {selected_axis: (word1, word2)}
            words_to_test = [w.strip() for w in test_words.split(',') if w.strip()]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª–æ–≤–∞
            available_test_words = [w for w in words_to_test if w in self.selected_model.wv.key_to_index]
            missing_test_words = [w for w in words_to_test if w not in self.selected_model.wv.key_to_index]
            
            if missing_test_words:
                st.warning(f"–ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {', '.join(missing_test_words[:5])}")
            
            if len(available_test_words) < 3:
                st.error("–°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞–π–¥–µ–Ω–æ –≤ –º–æ–¥–µ–ª–∏")
                return
            
            with st.spinner("–ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Å–∏..."):
                try:
                    if use_advanced:
                        results = self.semantic_analyzer.analyze_semantic_axes_advanced(
                            self.selected_model, axis_pairs, available_test_words
                        )
                    else:
                        results = self.semantic_analyzer.analyze_semantic_axes(
                            self.selected_model, axis_pairs, available_test_words
                        )
                    
                    if selected_axis in results:
                        axis_result = results[selected_axis]
                        projections = axis_result['projections']
                        
                        if not projections:
                            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –ø—Ä–æ–µ–∫—Ü–∏–∏")
                            return
                        
                        # –°–û–†–¢–ò–†–£–ï–ú –ø–æ –ø—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
                        sorted_projections = sorted(projections.items(), key=lambda x: x[1])
                        
                        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                        st.subheader("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –≤–¥–æ–ª—å –æ—Å–∏")
                        
                        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                        df = pd.DataFrame([
                            {'word': word, 'projection': projection, 'abs_projection': abs(projection)}
                            for word, projection in sorted_projections
                        ])
                        
                        # –¶–≤–µ—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–æ–∑–∏—Ü–∏–∏
                        df['color'] = df['projection'].apply(
                            lambda x: 'red' if x < -0.1 else 'green' if x > 0.1 else 'gray'
                        )
                        
                        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                        fig = px.scatter(
                            df, x='projection', y=[0]*len(df),
                            text='word', color='color',
                            title=f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –æ—Å—å: '{word1}' ‚Üê‚Üí '{word2}'",
                            labels={'projection': '–ü–æ–∑–∏—Ü–∏—è –Ω–∞ –æ—Å–∏', 'y': ''},
                            color_discrete_map={'red': 'red', 'green': 'green', 'gray': 'gray'}
                        )
                        
                        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞
                        fig.update_traces(
                            marker=dict(size=15, opacity=0.7),
                            textposition='top center'
                        )
                        fig.update_layout(
                            showlegend=False,
                            yaxis=dict(showticklabels=False),
                            height=500
                        )
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–µ–Ω—Ç–∏—Ä—ã
                        fig.add_vline(x=-0.5, line_dash="dash", line_color="red", 
                                    annotation_text=word1, annotation_position="top left")
                        fig.add_vline(x=0.5, line_dash="dash", line_color="green",
                                    annotation_text=word2, annotation_position="top right")
                        fig.add_vline(x=0, line_dash="dot", line_color="gray")
                        
                        st.plotly_chart(fig)
                        
                        # –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
                        st.subheader("üìã –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
                        
                        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é
                        interpretation_df = pd.DataFrame([
                            {
                                '–°–ª–æ–≤–æ': word,
                                '–ü—Ä–æ–µ–∫—Ü–∏—è': f"{projection:.4f}",
                                '–ü–æ–∑–∏—Ü–∏—è': (
                                    f"–±–ª–∏–∂–µ –∫ '{word1}'" if projection < -0.1 else
                                    f"–±–ª–∏–∂–µ –∫ '{word2}'" if projection > 0.1 else
                                    "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"
                                ),
                                '–ê–±—Å. –∑–Ω–∞—á–µ–Ω–∏–µ': f"{abs(projection):.4f}"
                            }
                            for word, projection in sorted_projections
                        ])
                        
                        st.dataframe(interpretation_df, use_container_width=True)
                        
                        # –ì–†–£–ü–ü–ò–†–û–í–ö–ê
                        st.subheader("üéØ –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–ª–æ–≤")
                        
                        left_words = [word for word, proj in sorted_projections if proj < -0.1]
                        neutral_words = [word for word, proj in sorted_projections if -0.1 <= proj <= 0.1]
                        right_words = [word for word, proj in sorted_projections if proj > 0.1]
                        
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric(f"–ë–ª–∏–∂–µ –∫ '{word1}'", len(left_words))
                            if left_words:
                                for word in left_words:
                                    st.write(f"‚Ä¢ {word}")
                        
                        with col2:
                            st.metric("–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ", len(neutral_words))
                            if neutral_words:
                                for word in neutral_words:
                                    st.write(f"‚Ä¢ {word}")
                        
                        with col3:
                            st.metric(f"–ë–ª–∏–∂–µ –∫ '{word2}'", len(right_words))
                            if right_words:
                                for word in right_words:
                                    st.write(f"‚Ä¢ {word}")
                        
                        # –û–¢–õ–ê–î–û–ß–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø
                        if show_debug:
                            st.subheader("üîß –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
                            
                            # –°—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –∫—Ä–∞–π–Ω–∏–º–∏ —Ç–æ—á–∫–∞–º–∏
                            similarity = axis_result.get('similarity_between_ends', 0)
                            st.write(f"–°—Ö–æ–¥—Å—Ç–≤–æ '{word1}' –∏ '{word2}': {similarity:.4f}")
                            
                            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Å–∏
                            bias_analysis = axis_result.get('bias_analysis', {})
                            if bias_analysis:
                                st.write("**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Å–∏:**")
                                st.write(f"- –°—Ä–µ–¥–Ω–µ–µ: {bias_analysis.get('mean', 0):.4f}")
                                st.write(f"- –î–∏–∞–ø–∞–∑–æ–Ω: {bias_analysis.get('range', 0):.4f}")
                                st.write(f"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {bias_analysis.get('count', 0)}")
                            
                            # –ü—Ä–∏–º–µ—Ä—ã –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
                            st.write("**–ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏:**")
                            col1, col2 = st.columns(2)
                            with col1:
                                neighbors1 = self.semantic_analyzer.find_similar_words(self.selected_model, word1, 5)
                                if neighbors1:
                                    st.write(f"–ö '{word1}':")
                                    for neighbor, sim in neighbors1:
                                        st.write(f"  - {neighbor} ({sim:.3f})")
                            with col2:
                                neighbors2 = self.semantic_analyzer.find_similar_words(self.selected_model, word2, 5)
                                if neighbors2:
                                    st.write(f"–ö '{word2}':")
                                    for neighbor, sim in neighbors2:
                                        st.write(f"  - {neighbor} ({sim:.3f})")
                    
                    else:
                        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Å—å")
                        
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –æ—Å–∏: {e}")
                    st.info("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ—Å–∏")
    
    def render_visualization(self):
        """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å 2D/3D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏"""
        st.header("üé® 2D/3D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤")
        
        col1, col2 = st.columns(2)
        
        with col1:
            visualization_type = st.selectbox(
                "–¢–∏–ø –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:",
                ["t-SNE", "UMAP"]
            )
            n_components = st.selectbox("–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å:", [2, 3], index=0)
        
        with col2:
            sample_size = st.slider("–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏:", 100, 1000, 300)
            perplexity = st.slider("Perplexity (t-SNE):", 5, 50, 30) if visualization_type == "t-SNE" else 15
        
        if st.button("–í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å") and self.selected_model:
            with st.spinner("–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è..."):
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤
                    words = list(self.selected_model.wv.key_to_index.keys())[:sample_size]
                    vectors = np.array([self.selected_model.wv[word] for word in words])
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                    if visualization_type == "t-SNE":
                        embeddings = self.dimensionality_reducer.apply_tsne(
                            vectors, n_components=n_components, perplexity=perplexity
                        )
                    else:
                        embeddings = self.dimensionality_reducer.apply_umap(
                            vectors, n_components=n_components
                        )
                    
                    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    n_clusters = min(10, len(vectors))
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                    cluster_labels = kmeans.fit_predict(vectors)
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                    fig = self.dimensionality_reducer.visualize_embeddings(
                        embeddings, 
                        labels=words,
                        cluster_labels=[f"–ö–ª–∞—Å—Ç–µ—Ä {label}" for label in cluster_labels]
                    )
                    
                    st.plotly_chart(fig)
                    
                    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
                    st.subheader("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö")
                    cluster_info = []
                    for cluster_id in range(n_clusters):
                        cluster_words = [words[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
                        if cluster_words:
                            cluster_info.append({
                                '–ö–ª–∞—Å—Ç–µ—Ä': cluster_id,
                                '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤': len(cluster_words),
                                '–ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤': ', '.join(cluster_words[:5])
                            })
                    
                    st.table(pd.DataFrame(cluster_info))
                    
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
    
    def render_dynamic_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        st.header("üìã –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç")
        
        if not self.selected_model:
            st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞")
            return
        
        if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á–µ—Ç"):
            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞..."):
                try:
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏
                    st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏")
                    vocab_size = len(self.selected_model.wv.key_to_index)
                    vector_size = self.selected_model.wv.vector_size
                    
                    col1, col2, col3 = st.columns(3)
                    col1.metric("–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è", vocab_size)
                    col2.metric("–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤", vector_size)
                    
                    # –¢–æ—á–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–æ–≥–∏–π
                    analogies_result = DistributedModels().evaluate_word_analogies(self.selected_model)
                    col3.metric("–¢–æ—á–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–æ–≥–∏–π", f"{analogies_result.get('accuracy', 0):.2%}")
                    
                    # –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞ - –ø—Ä–∏–º–µ—Ä—ã
                    st.subheader("üßÆ –ü—Ä–∏–º–µ—Ä—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏")
                    test_cases = [
                        ("—Å—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏ - –ú–æ—Å–∫–≤–∞ + –§—Ä–∞–Ω—Ü–∏—è", ["–ø–∞—Ä–∏–∂", "—Ñ—Ä–∞–Ω—Ü–∏—è"], ["–º–æ—Å–∫–≤–∞", "—Ä–æ—Å—Å–∏—è"]),
                        ("–∫–æ—Ä–æ–ª—å - –º—É–∂—á–∏–Ω–∞ + –∂–µ–Ω—â–∏–Ω–∞", ["–∫–æ—Ä–æ–ª—å", "–∂–µ–Ω—â–∏–Ω–∞"], ["–º—É–∂—á–∏–Ω–∞"]),
                        ("—Ö–æ–ª–æ–¥–Ω—ã–π - –ª–µ—Ç–æ + –∑–∏–º–∞", ["—Ö–æ–ª–æ–¥–Ω—ã–π", "–∑–∏–º–∞"], ["–ª–µ—Ç–æ"])
                    ]
                    
                    for description, positive, negative in test_cases:
                        with st.expander(description):
                            result = self.semantic_analyzer.vector_arithmetic(
                                self.selected_model, positive, negative, topn=5
                            )
                            if result:
                                final_df = pd.DataFrame(result, 
                                                      columns=['–°–ª–æ–≤–æ', '–°—Ö–æ–¥—Å—Ç–≤–æ'])
                                st.table(final_df)
                            else:
                                st.write("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å")
                    
                    # Heatmap —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–∏–∑–æ—Å—Ç–µ–π
                    st.subheader("üî• Heatmap —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–∏–∑–æ—Å—Ç–µ–π (—Ç–æ–ø-20 —Å–ª–æ–≤)")
                    top_words = list(self.selected_model.wv.key_to_index.keys())[:20]
                    if len(top_words) >= 2:
                        heatmap_fig = self.semantic_analyzer.create_similarity_heatmap(
                            self.selected_model, top_words
                        )
                        st.plotly_chart(heatmap_fig)
                    else:
                        st.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è heatmap")
                    
                    # 2D –ø—Ä–æ–µ–∫—Ü–∏—è
                    st.subheader("üé® 2D –ø—Ä–æ–µ–∫—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞")
                    words_sample = list(self.selected_model.wv.key_to_index.keys())[:100]
                    if len(words_sample) >= 2:
                        vectors = np.array([self.selected_model.wv[word] for word in words_sample])
                        
                        embeddings = self.dimensionality_reducer.apply_tsne(vectors, n_components=2)
                        
                        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                        n_clusters = min(5, len(vectors))
                        if n_clusters > 1:
                            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                            cluster_labels = kmeans.fit_predict(vectors)
                            
                            fig = self.dimensionality_reducer.visualize_embeddings(
                                embeddings, 
                                labels=words_sample,
                                cluster_labels=[f"–ö–ª–∞—Å—Ç–µ—Ä {label}" for label in cluster_labels]
                            )
                            st.plotly_chart(fig)
                        else:
                            st.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
                    else:
                        st.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
                    
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
    
    def run(self):
        """–ó–∞–ø—É—Å–∫ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        st.set_page_config(
            page_title="–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤",
            page_icon="üîç",
            layout="wide"
        )
        
        page = self.render_sidebar()
        
        if page == "–í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞":
            self.render_vector_arithmetic()
        elif page == "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ":
            self.render_semantic_similarity()
        elif page == "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–∏":
            self.render_semantic_axes()
        elif page == "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è":
            self.render_visualization()
        elif page == "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç":
            self.render_dynamic_report()