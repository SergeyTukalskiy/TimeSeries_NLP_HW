import os
import sys
import json
import pandas as pd
from typing import List, Dict, Any

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from utils import TextPreprocessor, CorpusLoader
from classical_vectorizers import ClassicalVectorizers
from dimensionality_reduction import DimensionalityReducer
from distributed_models import DistributedModels
from semantic_analysis import SemanticAnalyzer

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    preprocessor = TextPreprocessor()
    corpus_loader = CorpusLoader(preprocessor)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–ø—É—Å–∞
    print("\nüìÅ –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–ø—É—Å–∞")
    corpus = corpus_loader.load_corpus('data/rbc_articles_words.jsonl')
    processed_corpus = corpus_loader.process_corpus(corpus)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
    corpus_loader.save_processed_corpus(processed_corpus, 'data/processed_corpus.jsonl')
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä–µ–º–∞ –∫–æ—Ä–ø—É—Å–∞
    total_words = sum(doc['word_count'] for doc in processed_corpus)
    print(f"üìä –û–±—â–∏–π –æ–±—ä–µ–º –∫–æ—Ä–ø—É—Å–∞: {total_words} —Å–ª–æ–≤")
    print(f"üìÑ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(processed_corpus)}")
    
    # 2. –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    print("\nüî¢ –≠—Ç–∞–ø 2: –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
    vectorizers = ClassicalVectorizers()
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤
    comparison_df = vectorizers.compare_methods(processed_corpus)
    comparison_df.to_csv('vectorization_metrics.csv', index=False, encoding='utf-8')
    print("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ vectorization_metrics.csv")
    
    # 3. –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    print("\nüìâ –≠—Ç–∞–ø 3: –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏")
    reducer = DimensionalityReducer()
    
    # –ü–æ–ª—É—á–∞–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    tfidf_result = vectorizers.tfidf_vectorizer(processed_corpus)
    tfidf_matrix = tfidf_result['matrix']
    
    # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
    optimal_components = reducer.find_optimal_components(tfidf_matrix)
    print(f"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç: {optimal_components['optimal_components']}")
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SVD
    svd_result = reducer.apply_svd(tfidf_matrix, n_components=100)
    print(f"üìä –û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è: {svd_result['explained_variance']:.4f}")
    
    # 4. –ú–æ–¥–µ–ª–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
    print("\nüß† –≠—Ç–∞–ø 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    sentences = [doc['processed_text'] for doc in processed_corpus]
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    dist_models = DistributedModels(vector_size=100, window=5, min_count=5)
    
    # Word2Vec
    w2v_skipgram = dist_models.train_word2vec(sentences, sg=1)
    w2v_cbow = dist_models.train_word2vec(sentences, sg=0)
    
    # FastText (–∏—Å–ø–æ–ª—å–∑—É–µ–º gensim)
    ft_skipgram = dist_models.train_fasttext(sentences, sg=1)
    ft_cbow = dist_models.train_fasttext(sentences, sg=0)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    models_comparison = dist_models.compare_models(sentences, processed_corpus)
    models_comparison.to_csv('models_comparison.csv', index=False, encoding='utf-8')
    print("‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ models_comparison.csv")
    
    # 5. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    print("\nüîç –≠—Ç–∞–ø 5: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑")
    semantic_analyzer = SemanticAnalyzer()
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏
    test_analogies = [
        (['–º–æ—Å–∫–≤–∞', '—Ñ—Ä–∞–Ω—Ü–∏—è'], ['—Ä–æ—Å—Å–∏—è'], '–ø–∞—Ä–∏–∂'),
        (['–∫–æ—Ä–æ–ª—å', '–∂–µ–Ω—â–∏–Ω–∞'], ['–º—É–∂—á–∏–Ω–∞'], '–∫–æ—Ä–æ–ª–µ–≤–∞')
    ]
    
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏:")
    for pos, neg, expected in test_analogies:
        result = semantic_analyzer.vector_arithmetic(w2v_skipgram, pos, neg, topn=3)
        if result:
            print(f"  {pos} - {neg} = {result[0][0]} (–æ–∂–∏–¥–∞–ª–æ—Å—å: {expected})")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
    models_dir = 'models'
    os.makedirs(models_dir, exist_ok=True)
    
    w2v_skipgram.save(os.path.join(models_dir, 'word2vec_skipgram.model'))
    w2v_cbow.save(os.path.join(models_dir, 'word2vec_cbow.model'))
    ft_skipgram.save(os.path.join(models_dir, 'fasttext_skipgram.model'))
    ft_cbow.save(os.path.join(models_dir, 'fasttext_cbow.model'))
    
    print("‚úÖ –í—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
    
    return {
        'processed_corpus': processed_corpus,
        'vectorizers': vectorizers,
        'reducer': reducer,
        'distributed_models': dist_models,
        'semantic_analyzer': semantic_analyzer
    }

if __name__ == "__main__":
    results = main()