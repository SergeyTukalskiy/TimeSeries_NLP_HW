from huggingface_hub import HfApi, ModelCard
import os


class ModelPublisher:
    def __init__(self, username, token):
        self.api = HfApi()
        self.username = username
        self.token = token

    def create_model_card(self, model_name, metrics, corpus_info):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –º–æ–¥–µ–ª–∏"""

        card_content = f"""
---
language:
- ru
license: mit
tags:
- russian
- tokenizer
- nlp
- BPE
---

# {model_name}

## üóÉÔ∏è –ö–æ—Ä–ø—É—Å
{corpus_info}

## ‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
- –ê–ª–≥–æ—Ä–∏—Ç–º: BPE
- –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {metrics.get('vocab_size', 'N/A')}
- Min frequency: {metrics.get('min_frequency', 'N/A')}

## üìä –ú–µ—Ç—Ä–∏–∫–∏
- OOV rate: {metrics.get('oov_rate', 'N/A')}%
- Reconstruction accuracy: {metrics.get('reconstruction_accuracy', 'N/A')}%
- Compression ratio: {metrics.get('compression_ratio', 'N/A')}

## üíª –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("{self.username}/{model_name}")
text = "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"
tokens = tokenizer.tokenize(text)
print(tokens)"""