import json

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–∏–º –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(__file__))


class TokenizationWebApp:
    def __init__(self):
        self.setup_page()

    def setup_page(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Streamlit"""
        st.set_page_config(
            page_title="–ê–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏",
            page_icon="üìä",
            layout="wide"
        )

        st.title("–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
        st.markdown("---")

    def run(self):
        """–ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        st.sidebar.header("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")

        uploaded_file = st.sidebar.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ JSONL —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–∞–º–∏",
            type=['jsonl']
        )

        # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–æ–≤
        st.sidebar.header("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞")

        selected_methods = st.sidebar.multiselect(
            "–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:",
            ["Naive", "Razdel", "SpaCy", "Pymorphy Lemma", "Snowball Stem", "BPE", "WordPiece"]
        )

        # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
        if uploaded_file is not None:
            self.analyze_data(uploaded_file, selected_methods)
        else:
            self.show_instructions()

    def show_instructions(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        st.info("""
        ### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:
        1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ JSONL —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
        2. –í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        3. –ü—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∞—Ö
        4. –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –æ—Ç—á—ë—Ç –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        """)

        # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
        st.subheader("–ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã JSONL —Ñ–∞–π–ª–∞:")
        st.code("""
        {"title": "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏", "text": "–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏...", "date": "2024-01-01", "url": "https://..."}
        {"title": "–î—Ä—É–≥–∞—è –Ω–æ–≤–æ—Å—Ç—å", "text": "–ï—â–µ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç...", "date": "2024-01-02", "url": "https://..."}
        """)

    def analyze_data(self, uploaded_file, selected_methods):
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        texts = self.load_data(uploaded_file)

        if not texts:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –∏–∑ —Ñ–∞–π–ª–∞")
            return

        # –ü–æ–∫–∞–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.show_statistics(texts)

        if selected_methods:
            # –ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤
            comparison_results = self.run_comparison(texts, selected_methods)

            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.visualize_results(comparison_results, texts)

            # –ö–Ω–æ–ø–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
            if st.button("–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á—ë—Ç –≤ HTML"):
                self.export_report(comparison_results)

    def load_data(self, uploaded_file):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSONL"""
        texts = []
        try:
            for line in uploaded_file:
                data = json.loads(line)
                if 'text' in data:
                    texts.append(data['text'])
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return texts

    def show_statistics(self, texts):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ—Ä–ø—É—Å–∞"""
        col1, col2, col3, col4 = st.columns(4)

        total_texts = len(texts)
        total_words = sum(len(text.split()) for text in texts)
        avg_words = total_words / total_texts if total_texts > 0 else 0

        with col1:
            st.metric("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤", total_texts)
        with col2:
            st.metric("–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤", total_words)
        with col3:
            st.metric("–°—Ä–µ–¥–Ω–µ–µ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ", f"{avg_words:.1f}")
        with col4:
            st.metric("–†–∞–∑–º–µ—Ä –∫–æ—Ä–ø—É—Å–∞", f"{(sum(len(t.encode('utf-8')) for t in texts) / 1024 / 1024):.1f} MB")

    def run_comparison(self, texts, methods):
        """–ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤"""
        # –ó–¥–µ—Å—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –≤–∞—à –∫–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤
        # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä
        results = {}

        for method in methods:
            # –ò–º–∏—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            results[method] = {
                'vocab_size': len(set(' '.join(texts).split())),  # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç
                'processing_time': 1.0,
                'oov_rate': 0.05,
                'fragmentation_rate': 0.1
            }

        return results

    def visualize_results(self, results, texts):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        st.subheader("–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤")

        # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        df = pd.DataFrame.from_dict(results, orient='index')

        col1, col2 = st.columns(2)

        with col1:
            fig = px.bar(df, y='vocab_size', title='–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è')
            st.plotly_chart(fig)

        with col2:
            fig = px.bar(df, y='oov_rate', title='OOV Rate')
            st.plotly_chart(fig)

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–æ–∫–µ–Ω–æ–≤
        st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–æ–∫–µ–Ω–æ–≤")
        # –î–æ–±–∞–≤—å—Ç–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

        # –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤
        st.subheader("–¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤")
        self.show_top_tokens(texts)

    def show_top_tokens(self, texts):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ø —Ç–æ–∫–µ–Ω–æ–≤"""
        all_tokens = ' '.join(texts).split()
        token_freq = Counter(all_tokens)
        top_tokens = token_freq.most_common(20)

        tokens, counts = zip(*top_tokens)
        fig = px.bar(x=counts, y=tokens, orientation='h', title='–¢–æ–ø-20 —Ç–æ–∫–µ–Ω–æ–≤')
        st.plotly_chart(fig)

    def export_report(self, results):
        """–≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–∞"""
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ HTML/PDF
        st.success("–û—Ç—á—ë—Ç —É—Å–ø–µ—à–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω!")


if __name__ == "__main__":
    app = TokenizationWebApp()
    app.run()