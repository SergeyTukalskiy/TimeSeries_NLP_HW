import requests as rq
from bs4 import BeautifulSoup as bs
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import re
import json
from collections import Counter
import os
from typing import List, Dict, Optional


class RbcParser:
    def __init__(self):
        self.total_words = 0
        self.articles_collected = 0
        self.processed_urls = set()  # –î–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

    def _count_words(self, text):
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not text:
            return 0
        words = re.findall(r'\b\w+\b', text)
        return len(words)

    def _get_url(self, param_dict: dict) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ json —Ç–∞–±–ª–∏—Ü—ã —Å–æ —Å—Ç–∞—Ç—å—è–º–∏
        """
        url = 'https://www.rbc.ru/search/ajax/?' + \
              'project={0}&'.format(param_dict['project']) + \
              'category={0}&'.format(param_dict['category']) + \
              'dateFrom={0}&'.format(param_dict['dateFrom']) + \
              'dateTo={0}&'.format(param_dict['dateTo']) + \
              'page={0}&'.format(param_dict['page']) + \
              'query={0}&'.format(param_dict['query']) + \
              'material={0}'.format(param_dict['material'])

        return url

    def _get_search_results(self, param_dict: dict) -> list:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π —Å –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–∏
        """
        url = self._get_url(param_dict)
        try:
            r = rq.get(url)
            r.raise_for_status()
            return r.json()['items']
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–∏: {e}")
            return []

    def _get_article_data(self, url: str):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫, —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏, –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –¥–∞—Ç—É –ø–æ —Å—Å—ã–ª–∫–µ
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            r = rq.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            soup = bs(r.text, features="lxml")

            # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = soup.find('h1') or soup.find('title')
            title = title_elem.text.strip() if title_elem else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"

            # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            date = None
            date_elem = soup.find('time')
            if date_elem and date_elem.get('datetime'):
                date = date_elem.get('datetime')
            else:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –Ω–∞–π—Ç–∏ –¥–∞—Ç—É
                date_span = soup.find('span', {'class': 'article__header__date'})
                if date_span:
                    date = date_span.text.strip()

            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
            text = ""
            article_body = soup.find('div', {'class': 'article__text'})
            if not article_body:
                article_body = soup.find('article')

            if article_body:
                paragraphs = article_body.find_all('p')
                text = ' '.join([p.text.strip() for p in paragraphs if p.text.strip()])

            # –ü–æ–ª—É—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é/—Ä—É–±—Ä–∏–∫—É
            category = "–ù–µ —É–∫–∞–∑–∞–Ω–∞"
            breadcrumbs = soup.find('div', {'class': 'article__header__breadcrumbs'})
            if breadcrumbs:
                category_links = breadcrumbs.find_all('a')
                if category_links:
                    category = category_links[-1].text.strip()

            if category == "–ù–µ —É–∫–∞–∑–∞–Ω–∞":
                category_elem = soup.find('a', {'class': 'article__header__category'})
                if category_elem:
                    category = category_elem.text.strip()

            return title, text, category, date, url

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
            return None, None, None, None, None

    def _get_current_date_range(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –∏ –¥–∞—Ç—É –Ω–µ–¥–µ–ª—é –Ω–∞–∑–∞–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)
        return start_date.strftime('%d.%m.%Y'), end_date.strftime('%d.%m.%Y')

    def collect_articles(self,
                         word_limit: int = 50000,
                         delay: float = 1.0) -> List[Dict]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ç —Å–∞–º—ã—Ö —Å–≤–µ–∂–∏—Ö –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ —Å–ª–æ–≤

        Args:
            word_limit: –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50000)
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

        Returns:
            List[Dict] —Å —Å–æ–±—Ä–∞–Ω–Ω—ã–º–∏ —Å—Ç–∞—Ç—å—è–º–∏
        """
        articles_data = []
        self.total_words = 0
        self.articles_collected = 0
        self.processed_urls.clear()

        # –ù–∞—á–∏–Ω–∞–µ–º —Å —Ç–µ–∫—É—â–µ–π –¥–∞—Ç—ã –∏ –∏–¥–µ–º –≤–≥–ª—É–±—å
        current_end_date = datetime.now()

        while self.total_words < word_limit:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ (–Ω–µ–¥–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)
            start_date = current_end_date - timedelta(days=7)

            start_str = start_date.strftime('%d.%m.%Y')
            end_str = current_end_date.strftime('%d.%m.%Y')

            print(f"–ü–æ–∏—Å–∫ –∑–∞ –ø–µ—Ä–∏–æ–¥: {start_str} - {end_str}")

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
            params = {
                'project': 'rbcnews',
                'category': '',
                'dateFrom': start_str,
                'dateTo': end_str,
                'page': '1',
                'query': '',
                'material': 'news'
            }

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
            page = 1
            has_more_pages = True
            period_articles = 0

            while has_more_pages and self.total_words < word_limit:
                params['page'] = str(page)

                try:
                    search_results = self._get_search_results(params)

                    if not search_results:
                        has_more_pages = False
                        break

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç–∞—Ç—å—é –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                    for article in search_results:
                        if self.total_words >= word_limit:
                            break

                        url = article.get('fronturl')
                        if not url or url in self.processed_urls:
                            continue

                        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                        title, text, category, date, url = self._get_article_data(url)

                        if title and text and url:
                            word_count = self._count_words(text)

                            if word_count > 50:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç–∞—Ç—å–∏
                                article_dict = {
                                    'title': title,
                                    'text': text,
                                    'category': category,
                                    'date': date or article.get('publish_date', ''),
                                    'url': url,
                                    'source': 'rbc.ru'
                                }

                                articles_data.append(article_dict)
                                self.total_words += word_count
                                self.articles_collected += 1
                                period_articles += 1
                                self.processed_urls.add(url)

                                print(
                                    f"    –°—Ç–∞—Ç—å—è {self.articles_collected}: {word_count} —Å–ª–æ–≤ | –í—Å–µ–≥–æ: {self.total_words:,} —Å–ª–æ–≤")

                        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ —Å—Ç–∞—Ç—å—è–º
                        time.sleep(delay)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                    if len(search_results) < 20:  # –û–±—ã—á–Ω–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ 20 —Å—Ç–∞—Ç–µ–π
                        has_more_pages = False
                    else:
                        page += 1

                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
                    has_more_pages = False

            print(f"  –ó–∞ –ø–µ—Ä–∏–æ–¥ —Å–æ–±—Ä–∞–Ω–æ: {period_articles} —Å—Ç–∞—Ç–µ–π")

            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –Ω–µ–¥–µ–ª–µ
            current_end_date = start_date - timedelta(days=1)

            # –ï—Å–ª–∏ –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç–∞—Ç–µ–π, –≤–æ–∑–º–æ–∂–Ω–æ, –¥–æ—Å—Ç–∏–≥–ª–∏ –ø—Ä–µ–¥–µ–ª–∞ –∞—Ä—Ö–∏–≤–∞
            if period_articles == 0:
                print("–í–æ–∑–º–æ–∂–Ω–æ, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ø—Ä–µ–¥–µ–ª –∞—Ä—Ö–∏–≤–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
                break

        print("\n" + "=" * 50)
        print("–°–ë–û–† –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù")
        print(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {self.articles_collected}")
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {self.total_words:,}")
        print(f"–¶–µ–ª–µ–≤–æ–π –ª–∏–º–∏—Ç: {word_limit:,} —Å–ª–æ–≤")

        if self.total_words >= word_limit:
            print("‚úì –õ–∏–º–∏—Ç —Å–ª–æ–≤ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç!")
        else:
            print("‚ö† –õ–∏–º–∏—Ç —Å–ª–æ–≤ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç (–∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å —Å—Ç–∞—Ç—å–∏ –≤ –∞—Ä—Ö–∏–≤–µ)")

        return articles_data


class NewsCorpusCollector:
    def __init__(self):
        self.articles = []
        self.rbc_parser = RbcParser()

    def collect_from_rbc(self, word_limit: int = 50000, delay: float = 0.5) -> List[Dict]:
        """–°–±–æ—Ä —Å—Ç–∞—Ç–µ–π —Å RBC"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Å—Ç–∞—Ç–µ–π —Å RBC...")
        rbc_articles = self.rbc_parser.collect_articles(word_limit=word_limit, delay=delay)
        self.articles.extend(rbc_articles)
        return rbc_articles

    def collect_from_static_source(self, url: str, source: str) -> Optional[Dict]:
        """–ë–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = rq.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = bs(response.content, 'html.parser')

            # –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ - –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –∫–∞–∂–¥—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
            title = soup.find('h1')
            title = title.get_text().strip() if title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"

            # –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã)
            text_elements = soup.find_all(['p', 'div'], class_=re.compile(r'article|content|text|body'))
            text = ' '.join([elem.get_text().strip() for elem in text_elements if elem.get_text().strip()])

            if not text:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
                article = soup.find('article')
                if article:
                    text = article.get_text().strip()

            date = soup.find('time')
            date = date.get('datetime') if date and date.get('datetime') else None

            return {
                'title': title,
                'text': text,
                'date': date,
                'url': url,
                'source': source,
                'category': 'news'
            }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
            return None

    def collect_corpus(self, target_size: int = 50000, sources: List[str] = None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∞ –∫–æ—Ä–ø—É—Å–∞

        Args:
            target_size: —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä –∫–æ—Ä–ø—É—Å–∞ –≤ —Å–ª–æ–≤–∞—Ö
            sources: —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        """
        if sources is None:
            sources = ['rbc']  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–æ–ª—å–∫–æ RBC

        current_words = self.get_total_words()

        for source in sources:
            if current_words >= target_size:
                break

            if source.lower() == 'rbc':
                print(f"\nüì∞ –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source}")
                articles = self.collect_from_rbc(
                    word_limit=target_size - current_words,
                    delay=0.5  # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                )
                current_words = self.get_total_words()

            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            # elif source.lower() == 'ria':
            #     articles = self.collect_from_ria(...)

            print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ {source}")

    def get_total_words(self) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤–æ –≤—Å–µ—Ö —Å—Ç–∞—Ç—å—è—Ö"""
        total = 0
        for article in self.articles:
            if 'text' in article and article['text']:
                words = re.findall(r'\b\w+\b', article['text'])
                total += len(words)
        return total

    def get_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–æ—Ä–ø—É—Å—É"""
        if not self.articles:
            return {}

        total_articles = len(self.articles)
        total_words = self.get_total_words()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        sources = Counter(article.get('source', 'unknown') for article in self.articles)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = Counter(article.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–∞') for article in self.articles)

        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å—Ç–∞—Ç—å–∏
        avg_words_per_article = total_words / total_articles if total_articles > 0 else 0

        return {
            'total_articles': total_articles,
            'total_words': total_words,
            'avg_words_per_article': avg_words_per_article,
            'sources': dict(sources),
            'categories': dict(categories)
        }

    def save_to_jsonl(self, filename: str = 'raw_corpus.jsonl'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞ –≤ JSONL —Ñ–æ—Ä–º–∞—Ç"""
        with open(filename, 'w', encoding='utf-8') as f:
            for article in self.articles:
                f.write(json.dumps(article, ensure_ascii=False) + '\n')

        print(f"‚úÖ –ö–æ—Ä–ø—É—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filename}")

        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = self.get_statistics()
        if stats:
            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä–ø—É—Å–∞:")
            print(f"   –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {stats['total_articles']}")
            print(f"   –í—Å–µ–≥–æ —Å–ª–æ–≤: {stats['total_words']:,}")
            print(f"   –°—Ä–µ–¥–Ω–µ–µ —Å–ª–æ–≤ –≤ —Å—Ç–∞—Ç—å–µ: {stats['avg_words_per_article']:.1f}")
            print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {stats['sources']}")

    def load_from_jsonl(self, filename: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –∏–∑ JSONL —Ñ–∞–π–ª–∞"""
        try:
            self.articles = []
            with open(filename, 'r', encoding='utf-8') as f:
                for line in f:
                    article = json.loads(line.strip())
                    self.articles.append(article)

            print(f"‚úÖ –ö–æ—Ä–ø—É—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞: {filename}")
            stats = self.get_statistics()
            print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {stats['total_articles']} —Å—Ç–∞—Ç–µ–π, {stats['total_words']:,} —Å–ª–æ–≤")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ—Ä–ø—É—Å–∞: {e}")
            return False


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def create_sample_corpus(word_limit: int = 50000) -> NewsCorpusCollector:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
    collector = NewsCorpusCollector()
    collector.collect_corpus(target_size=word_limit)
    return collector


def quick_collect(output_file: str = 'rbc_corpus.jsonl', word_limit: int = 10000):
    """–ë—ã—Å—Ç—Ä—ã–π —Å–±–æ—Ä –Ω–µ–±–æ–ª—å—à–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å–±–æ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞...")
    collector = NewsCorpusCollector()
    collector.collect_from_rbc(word_limit=word_limit, delay=0.3)

    if collector.articles:
        collector.save_to_jsonl(output_file)
        return collector
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç—å–∏")
        return None


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
    collector = quick_collect(word_limit=5000)

    # –ü–æ–ª–Ω—ã–π —Å–±–æ—Ä
    # collector = create_sample_corpus(word_limit=50000)
    # collector.save_to_jsonl('full_corpus.jsonl')