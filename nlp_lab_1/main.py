import argparse
import sys
import os

import nltk

from config import Config
from corpus_collector import quick_collect, NewsCorpusCollector


def main(arg):
    parser = argparse.ArgumentParser(description="NLP Tokenization Analysis Pipeline")

    subparsers = parser.add_subparsers(dest='command', help='Available commands')

    # –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ—Ä–ø—É—Å–∞
    parse_parser = subparsers.add_parser('parse', help='Parse news corpus')
    parse_parser.add_argument('--size', type=int, default=50000, help='Target corpus size in words')

    # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    clean_parser = subparsers.add_parser('clean', help='Clean and preprocess corpus')

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤
    compare_parser = subparsers.add_parser('compare', help='Compare tokenization methods')
    compare_parser.add_argument('--sample', type=int, default=1000, help='Sample size for comparison')

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    train_parser = subparsers.add_parser('train', help='Train subword models')
    train_parser.add_argument('--vocab-size', type=int, nargs='+', default=[8000, 16000, 32000])

    # –ó–∞–ø—É—Å–∫ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
    web_parser = subparsers.add_parser('web', help='Start web interface')

    # –ü—É–±–ª–∏–∫–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
    publish_parser = subparsers.add_parser('publish', help='Publish models to Hugging Face')
    publish_parser.add_argument('--model', required=True, help='Model name to publish')

    args = parser.parse_args()

    if arg == 'parse':
        run_parsing(50000)
    elif arg == 'clean':
        run_cleaning()
    elif arg == 'compare':
        run_comparison(1000)
    elif arg == 'train':
        run_training([2000, 3500, 5000])
    elif arg == 'web':
        run_web_interface()
    elif arg == 'publish':
        run_publishing('bpe_16000')
    else:
        parser.print_help()


def run_parsing(target_size):
    """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ—Ä–ø—É—Å–∞"""
    print("üöÄ Starting corpus parsing...")
    from corpus_collector import NewsCorpusCollector

    collector = NewsCorpusCollector()
    collector.collect_corpus(target_size=target_size)
    collector.save_to_jsonl(Config.RAW_CORPUS_PATH)

    print(f"‚úÖ Corpus saved to {Config.RAW_CORPUS_PATH}")
    print(f"üìä Total words: {collector.get_total_words()}")


def run_cleaning():
    """–ó–∞–ø—É—Å–∫ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    print("üßπ Starting data cleaning...")
    from text_cleaner import TextCleaner
    from universal_preprocessor import UniversalPreprocessor
    import json

    cleaner = TextCleaner()
    preprocessor = UniversalPreprocessor()

    cleaned_articles = []

    with open(Config.RAW_CORPUS_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            article = json.loads(line)

            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            clean_text = cleaner.clean_pipeline(article['text'])

            # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
            processed_text = preprocessor.preprocess(clean_text)

            article['processed_text'] = processed_text
            cleaned_articles.append(article)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
    with open(Config.CLEAN_CORPUS_PATH, 'w', encoding='utf-8') as f:
        for article in cleaned_articles:
            f.write(json.dumps(article, ensure_ascii=False) + '\n')

    print(f"‚úÖ Cleaned corpus saved to {Config.CLEAN_CORPUS_PATH}")


def run_comparison(sample_size):
    """–ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤"""
    print("üìä Starting method comparison...")
    from tokenization_comparison import TokenizationComparator
    import json

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    texts = []
    with open(Config.CLEAN_CORPUS_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            article = json.loads(line)
            texts.append(article['processed_text'])

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤
    comparator = TokenizationComparator(texts[:sample_size])
    results = comparator.run_comparison(sample_size=min(sample_size, len(texts)))

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    import pandas as pd
    df = pd.DataFrame.from_dict(results, orient='index')
    df.to_csv('results/tokenization_comparison.csv')

    print("‚úÖ Comparison results saved to results/tokenization_comparison.csv")
    print(df)


def run_training(vocab_sizes):
    """–û–±—É—á–µ–Ω–∏–µ –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    print("ü§ñ Training subword models...")
    from subword_training import SubwordTokenizerTrainer
    import json

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    texts = []
    with open(Config.CLEAN_CORPUS_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            article = json.loads(line)
            if 'processed_text' in article and article['processed_text']:
                texts.append(article['processed_text'])
            elif 'text' in article and article['text']:
                texts.append(article['text'])

    if not texts:
        print("‚ùå No texts found for training")
        return

    print(f"üìù Loaded {len(texts)} texts for training")

    trainer = SubwordTokenizerTrainer(texts)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
    if not vocab_sizes:
        vocab_sizes = trainer.corpus_stats['recommended_sizes']
        print(f"üéØ Using recommended vocab sizes: {vocab_sizes}")

    for vocab_size in vocab_sizes:
        print(f"\nüîß Training models with vocab_size={vocab_size}")

        try:
            # BPE
            print("  Training BPE...")
            bpe_tokenizer = trainer.train_bpe(vocab_size=vocab_size)
            bpe_tokenizer.save(f"models/bpe_{vocab_size}.json")
            print(f"  ‚úÖ BPE saved: models/bpe_{vocab_size}.json")

            # WordPiece
            print("  Training WordPiece...")
            wp_tokenizer = trainer.train_wordpiece(vocab_size=vocab_size)
            wp_tokenizer.save(f"models/wordpiece_{vocab_size}.json")
            print(f"  ‚úÖ WordPiece saved: models/wordpiece_{vocab_size}.json")

            # Unigram - —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
            print("  Training Unigram...")
            unigram_result = trainer.train_unigram(vocab_size=vocab_size)
            # Unigram –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
            print(f"  ‚úÖ Unigram trained: vocab_size={unigram_result['vocab_size']}")

        except Exception as e:
            print(f"‚ùå Error training models with vocab_size={vocab_size}: {e}")
            continue

    print("‚úÖ All models trained")


def run_web_interface():
    """–ó–∞–ø—É—Å–∫ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    print("üåê Starting web interface...")
    print("Open http://localhost:8501 in your browser")

    # –ó–∞–ø—É—Å–∫ Streamlit
    os.system("streamlit run web_app.py")


def run_publishing(model_name):
    """–ü—É–±–ª–∏–∫–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π"""
    print(f"üì§ Publishing model {model_name}...")
    from model_publisher import ModelPublisher

    publisher = ModelPublisher(
        username=Config.HF_USERNAME,
        token=Config.HF_TOKEN
    )

    # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–∏
    metrics = {
        'vocab_size': 16000,
        'oov_rate': 1.2,
        'compression_ratio': 1.35
    }

    corpus_info = "50k+ words from rbc.ru"

    success = publisher.publish_model(
        model_path=f"models/{model_name}",
        model_name=model_name,
        metrics=metrics,
        corpus_info=corpus_info
    )

    if success:
        print(f"‚úÖ Model {model_name} published successfully!")
    else:
        print("‚ùå Failed to publish model")


if __name__ == "__main__":
    main('web')