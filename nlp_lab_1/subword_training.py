import json
import re
from collections import Counter
from tokenizers import Tokenizer
from tokenizers.models import BPE, WordPiece, Unigram
from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.normalizers import NFKC, Sequence
import sentencepiece as spm
import os
import tempfile
from typing import Dict, List, Any
import logging
import shutil

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SubwordTokenizerTrainer:
    def __init__(self, corpus_texts: List[str]):
        self.corpus_texts = [text for text in corpus_texts if text and isinstance(text, str)]
        self.corpus_file = "corpus.txt"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ—Ä–ø—É—Å –≤ —Ñ–∞–π–ª
        self._prepare_corpus()

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ—Ä–ø—É—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        self.corpus_stats = self._analyze_corpus()

    def _prepare_corpus(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"üìù –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –∏–∑ {len(self.corpus_texts)} —Ç–µ–∫—Å—Ç–æ–≤...")

        with open(self.corpus_file, 'w', encoding='utf-8') as f:
            for text in self.corpus_texts:
                # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                cleaned_text = re.sub(r'\s+', ' ', text.strip())
                if cleaned_text:
                    f.write(cleaned_text + '\n')

        logger.info(f"‚úÖ –ö–æ—Ä–ø—É—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {self.corpus_file}")

    def _analyze_corpus(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        logger.info("üìä –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞...")

        all_text = ' '.join(self.corpus_texts)
        words = re.findall(r'\b\w+\b', all_text)
        unique_words = set(words)

        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_words = len(words)
        total_chars = len(all_text)
        vocab_size_estimate = len(unique_words)

        logger.info(f"   –í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words:,}")
        logger.info(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {vocab_size_estimate:,}")
        logger.info(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_chars:,}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è Unigram)
        max_unigram_vocab = min(7000, vocab_size_estimate + 1000)

        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Å–ª–æ–≤–∞—Ä—è
        recommended_sizes = [
            min(2000, max_unigram_vocab),
            min(5000, max_unigram_vocab),
            min(8000, max_unigram_vocab)
        ]

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        recommended_sizes = sorted(set(recommended_sizes))

        return {
            'total_words': total_words,
            'unique_words': vocab_size_estimate,
            'total_chars': total_chars,
            'max_unigram_vocab': max_unigram_vocab,
            'recommended_sizes': recommended_sizes
        }

    def train_bpe(self, vocab_size: int = 5000) -> Tokenizer:
        """–û–±—É—á–µ–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        logger.info(f"üî§ –û–±—É—á–µ–Ω–∏–µ BPE –º–æ–¥–µ–ª–∏ —Å vocab_size={vocab_size}")

        tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
        tokenizer.pre_tokenizer = Whitespace()

        trainer = BpeTrainer(
            vocab_size=vocab_size,
            min_frequency=2,
            special_tokens=["[UNK]", "[PAD]", "[BOS]", "[EOS]"],
            show_progress=True
        )

        try:
            tokenizer.train([self.corpus_file], trainer)
            logger.info(f"‚úÖ BPE –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞, —Å–ª–æ–≤–∞—Ä—å: {tokenizer.get_vocab_size()} —Ç–æ–∫–µ–Ω–æ–≤")
            return tokenizer
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è BPE: {e}")
            raise

    def train_wordpiece(self, vocab_size: int = 5000) -> Tokenizer:
        """–û–±—É—á–µ–Ω–∏–µ WordPiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        logger.info(f"üî§ –û–±—É—á–µ–Ω–∏–µ WordPiece –º–æ–¥–µ–ª–∏ —Å vocab_size={vocab_size}")

        tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))
        tokenizer.pre_tokenizer = Whitespace()

        trainer = WordPieceTrainer(
            vocab_size=vocab_size,
            min_frequency=2,
            special_tokens=["[UNK]", "[PAD]", "[BOS]", "[EOS]"],
            show_progress=True
        )

        try:
            tokenizer.train([self.corpus_file], trainer)
            logger.info(f"‚úÖ WordPiece –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞, —Å–ª–æ–≤–∞—Ä—å: {tokenizer.get_vocab_size()} —Ç–æ–∫–µ–Ω–æ–≤")
            return tokenizer
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è WordPiece: {e}")
            raise

    def train_unigram(self, vocab_size: int = 5000) -> Dict[str, Any]:
        """–û–±—É—á–µ–Ω–∏–µ Unigram –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é SentencePiece"""
        logger.info(f"üî§ –û–±—É—á–µ–Ω–∏–µ Unigram –º–æ–¥–µ–ª–∏ —Å vocab_size={vocab_size}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ vocab_size –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π
        if vocab_size > self.corpus_stats['max_unigram_vocab']:
            logger.warning(
                f"‚ö† –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π vocab_size –¥–ª—è Unigram. –£–º–µ–Ω—å—à–∞–µ–º –¥–æ {self.corpus_stats['max_unigram_vocab']}")
            vocab_size = self.corpus_stats['max_unigram_vocab']

        model_prefix = f"unigram_model_{vocab_size}"

        try:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è SentencePiece
            spm.SentencePieceTrainer.train(
                input=self.corpus_file,
                model_prefix=model_prefix,
                vocab_size=vocab_size,
                character_coverage=0.9995,
                model_type='unigram',
                pad_id=0,
                unk_id=1,
                bos_id=2,
                eos_id=3,
                pad_piece='[PAD]',
                unk_piece='[UNK]',
                bos_piece='[BOS]',
                eos_piece='[EOS]',
                num_threads=4
            )

            # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            sp = spm.SentencePieceProcessor()
            sp.load(f'{model_prefix}.model')

            logger.info(f"‚úÖ Unigram –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞, —Å–ª–æ–≤–∞—Ä—å: {sp.get_piece_size()} —Ç–æ–∫–µ–Ω–æ–≤")

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏ –º–æ–¥–µ–ª—å, –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–∞—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            return {
                'model': sp,
                'model_files': {
                    'model_file': f'{model_prefix}.model',
                    'vocab_file': f'{model_prefix}.vocab'
                },
                'vocab_size': vocab_size
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è Unigram: {e}")
            # –ü—Ä–æ–±—É–µ–º —Å –º–µ–Ω—å—à–∏–º vocab_size
            if vocab_size > 3000:
                logger.info("üîÑ –ü—Ä–æ–±—É–µ–º –æ–±—É—á–∏—Ç—å —Å vocab_size=3000...")
                return self.train_unigram(3000)
            else:
                raise

    def train_all_models(self, vocab_sizes: List[int] = None) -> Dict[str, Any]:
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ —Å–ª–æ–≤–∞—Ä—è"""
        if vocab_sizes is None:
            vocab_sizes = self.corpus_stats['recommended_sizes']

        logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏ —Å–ª–æ–≤–∞—Ä—è: {vocab_sizes}")

        results = {
            'bpe': {},
            'wordpiece': {},
            'unigram': {},
            'corpus_stats': self.corpus_stats
        }

        for vocab_size in vocab_sizes:
            logger.info(f"\n{'=' * 50}")
            logger.info(f"üìè –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
            logger.info(f"{'=' * 50}")

            try:
                # BPE
                bpe_model = self.train_bpe(vocab_size)
                results['bpe'][vocab_size] = {
                    'model': bpe_model,
                    'vocab_size': vocab_size
                }

                # WordPiece
                wp_model = self.train_wordpiece(vocab_size)
                results['wordpiece'][vocab_size] = {
                    'model': wp_model,
                    'vocab_size': vocab_size
                }

                # Unigram
                unigram_result = self.train_unigram(vocab_size)
                results['unigram'][vocab_size] = unigram_result

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å vocab_size={vocab_size}: {e}")
                continue

        return results

    def evaluate_tokenizer(self, tokenizer, tokenizer_type: str, test_texts: List[str]) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö"""
        logger.info(f"üìä –û—Ü–µ–Ω–∫–∞ {tokenizer_type} —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")

        fragmentation_rates = []
        compression_ratios = []
        token_counts = []

        for text in test_texts[:100]:  # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ 100 —Ç–µ–∫—Å—Ç–∞—Ö
            if not text:
                continue

            original_words = len(re.findall(r'\b\w+\b', text))

            if tokenizer_type == 'unigram':
                # –î–ª—è SentencePiece
                tokens = tokenizer.encode_as_pieces(text)
            else:
                # –î–ª—è tokenizers
                tokens = tokenizer.encode(text).tokens

            token_count = len(tokens)

            # –ü—Ä–æ—Ü–µ–Ω—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (—Å–ª–æ–≤–∞, —Ä–∞–∑–±–∏—Ç—ã–µ –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞)
            fragmented_words = sum(1 for token in tokens if '##' in token or '‚ñÅ' in token)
            fragmentation_rate = fragmented_words / token_count if token_count > 0 else 0

            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è
            compression_ratio = original_words / token_count if token_count > 0 else 1

            fragmentation_rates.append(fragmentation_rate)
            compression_ratios.append(compression_ratio)
            token_counts.append(token_count)

        return {
            'avg_fragmentation_rate': sum(fragmentation_rates) / len(fragmentation_rates) if fragmentation_rates else 0,
            'avg_compression_ratio': sum(compression_ratios) / len(compression_ratios) if compression_ratios else 1,
            'avg_tokens_per_text': sum(token_counts) / len(token_counts) if token_counts else 0,
            'total_tokens_evaluated': sum(token_counts)
        }

    def save_models(self, models_dict: Dict[str, Any], output_dir: str = "models"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        os.makedirs(output_dir, exist_ok=True)

        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ {output_dir}...")

        saved_models = {}

        for model_type, vocab_models in models_dict.items():
            if model_type == 'corpus_stats':
                continue

            saved_models[model_type] = {}

            for vocab_size, model_data in vocab_models.items():
                if model_type in ['bpe', 'wordpiece']:
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ tokenizers –º–æ–¥–µ–ª–µ–π
                    filename = f"{output_dir}/{model_type}_{vocab_size}.json"
                    model_data['model'].save(filename)
                    saved_models[model_type][vocab_size] = {
                        'file': filename,
                        'vocab_size': vocab_size
                    }
                    logger.info(f"‚úÖ {model_type}_{vocab_size} —Å–æ—Ö—Ä–∞–Ω–µ–Ω")

                elif model_type == 'unigram':
                    # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã SentencePiece
                    model_files = model_data['model_files']
                    for file_type, src_file in model_files.items():
                        dst_file = f"{output_dir}/{model_type}_{vocab_size}.{file_type.split('_')[0]}"
                        if os.path.exists(src_file):
                            shutil.copy2(src_file, dst_file)
                            logger.info(f"‚úÖ {dst_file} —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω")

                    saved_models[model_type][vocab_size] = {
                        'model_file': f"{output_dir}/{model_type}_{vocab_size}.model",
                        'vocab_file': f"{output_dir}/{model_type}_{vocab_size}.vocab",
                        'vocab_size': vocab_size
                    }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ—Ä–ø—É—Å–∞
        stats_file = f"{output_dir}/corpus_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(models_dict['corpus_stats'], f, ensure_ascii=False, indent=2)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö
        models_info_file = f"{output_dir}/models_info.json"
        with open(models_info_file, 'w', encoding='utf-8') as f:
            json.dump(saved_models, f, ensure_ascii=False, indent=2)

        logger.info(f"‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}")
        return saved_models

    def load_model(self, model_type: str, vocab_size: int, models_dir: str = "models"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            if model_type in ['bpe', 'wordpiece']:
                # –ó–∞–≥—Ä—É–∑–∫–∞ tokenizers –º–æ–¥–µ–ª–µ–π
                model_path = f"{models_dir}/{model_type}_{vocab_size}.json"
                tokenizer = Tokenizer.from_file(model_path)
                return tokenizer

            elif model_type == 'unigram':
                # –ó–∞–≥—Ä—É–∑–∫–∞ SentencePiece –º–æ–¥–µ–ª–∏
                model_path = f"{models_dir}/{model_type}_{vocab_size}.model"
                sp = spm.SentencePieceProcessor()
                sp.load(model_path)
                return sp

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_type}_{vocab_size}: {e}")
            return None


def load_corpus_from_jsonl(filename: str) -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –∏–∑ JSONL —Ñ–∞–π–ª–∞"""
    import json
    corpus = []

    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    article = json.loads(line.strip())
                    if 'text' in article and article['text']:
                        corpus.append(article['text'])
                except json.JSONDecodeError as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ JSON –≤ —Å—Ç—Ä–æ–∫–µ: {e}")
                    continue

        logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(corpus)} —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ {filename}")
        return corpus

    except FileNotFoundError:
        logger.error(f"‚ùå –§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []


def quick_train_test():
    """–ë—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–º –∫–æ—Ä–ø—É—Å–µ"""
    logger.info("üß™ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")

    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_texts = [
                     "–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.",
                     "–ó–¥–µ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.",
                     "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞.",
                     "–ú—ã —Ç–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.",
                     "–ü–æ–¥—Å–ª–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–º–æ–≥–∞—é—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞."
                 ] * 20  # –£–º–Ω–æ–∂–∞–µ–º –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ–±—ä–µ–º–∞

    logger.info(f"üìù –¢–µ—Å—Ç–æ–≤—ã–π –∫–æ—Ä–ø—É—Å: {len(test_texts)} —Ç–µ–∫—Å—Ç–æ–≤")

    try:
        trainer = SubwordTokenizerTrainer(test_texts)

        # –û–±—É—á–∞–µ–º —Å –º–∞–ª–µ–Ω—å–∫–∏–º —Å–ª–æ–≤–∞—Ä–µ–º
        results = trainer.train_all_models(vocab_sizes=[1000, 2000])

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        saved_models = trainer.save_models(results, "test_models")

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ
        test_text = "–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ–¥—Å–ª–æ–≤–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."

        for model_type, vocab_models in saved_models.items():
            for vocab_size, model_info in vocab_models.items():
                logger.info(f"\nüîç –¢–µ—Å—Ç {model_type}_{vocab_size}:")

                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                model = trainer.load_model(model_type, vocab_size, "test_models")
                if model:
                    if model_type == 'unigram':
                        tokens = model.encode_as_pieces(test_text)
                    else:
                        tokens = model.encode(test_text).tokens

                    logger.info(f"   –¢–µ–∫—Å—Ç: {test_text}")
                    logger.info(f"   –¢–æ–∫–µ–Ω—ã: {tokens}")
                    logger.info(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")

        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±—ã—Å—Ç—Ä–æ–º —Ç–µ—Å—Ç–µ: {e}")
        return False


if __name__ == "__main__":
    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
    if quick_train_test():
        logger.info("‚úÖ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    else:
        logger.error("‚ùå –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —Å –æ—à–∏–±–∫–∞–º–∏")